Glossary in English
=======================

## Abstract / Abstractness

##### Definition
1) (historical): Referring to the quality of a concept (or word meaning) that has no sensory or motor salience (in opposition to concrete) in that it cannot be seen, heard, touched, felt, smelled, tasted or acted upon; 
2) (contemporary): The quality of a concept (or word) whose meaning is understood primarily on the basis of language, but also draws from interoceptive experiences, including emotion, introspection, and metacognition. Abstract concepts are often exemplified by perceptually dissimilar associated actions and events. 

###### Background
```{toggle}
The traditional definition of abstractness corresponds to the people’s understanding of abstract vs. concrete, as revealed in subjective rating studies 
{cite}`paivio1968concreteness`. For most words, there is high agreement among participants about the degree to which the words refer to abstract or concrete concepts. There is also high agreement across rating studies, for instance, agreement between the ratings collected by {cite}`brysbaert2014concreteness` and the MRC ratings {cite}`coltheart1981mrc`, despite differences in instructions given to the participants. 

The reason to doubt an abstract vs. concrete bipolar dimension in the semantic system is that there is no opposition between language-based and experience-based information. Both sources of information correlate positively with each other and complement each other. For instance, it is possible to produce viable concreteness ratings with embedding-based semantic vectors derived from language corpora (Hollis et al., 2017). Therefore, most information based on experience can also be retrieved based on language use. Some argue that language-based information may be easier to activate, so that the meaning of concrete words is often predominantly based on language information, as it is for abstract words (Gatti et al., 2022; Louwerse, 2018). 

Although no formally articulated dichotomous opposition exists, it is widely acknowledged that concrete and abstract concepts both vary along numerous dimensions (Banks & Connell, 2023; Barsalou et al., 2018; Crutch et al., 2013; Reilly et al., 2016; Shallice & Cooper, 2013). Abstract words typically refer to multiple interacting elements rather than a single element. For example, the concept of "cause" includes at least one agent, an action, and at least one patient. It has been argued that abstract words differ in their network organization relative concrete words, with abstract words characterized by higher contextual and thematic/associative salience (Cousins et al., 2016; Crutch & Warrington, 2005) and lower taxonomic cohesion (for recent work on taxonomic relationships among abstract words, see Persichetti, Shao, Denning, Gotts, and Martin, 2024). More diffuse organization among abstract words is also associated with lower perceptual similarity among their associative lexical networks (Henningsen-Schomers & Pulvermüller, 2022; Langland-Hassan et al., 2021; Lupyan & Mirman, 2013; Borghi, 2022; Borghi et al., 2019). 

Abstract words are typically regarded as hard words (Gleitman et al., 2005), and these disproportionate difficulties for abstract words are typically manifested across many domains, including reading, spelling, word recognition, and serial recall (Fini et al., 2021; Sadoski et al., 1997; Sadoski & Paivio, 1994; Villani et al., 2022; Walker & Hulme, 1999). In addition to these objective performance discrepancies, people have reported lower confidence in understanding abstract word meanings and a stronger need for social didactic interactions with other people to acquire abstract word meanings (Fini et al., 2021; Mazzuca et al., 2022; Villani et al., 2019). Words referring to abstract concepts are typically acquired later (Della Rosa et al., 2010; Montefinese et al., 2019; Ponari et al., 2018; Ramey et al., 2013; Reilly & Kean, 2007). In addition, it is thought that abstract words are learned primarily via linguistic input (e.g., definitions, co-occurrence statistics) relative to concrete words that are dually coded both in the language system but also with sensorimotor grounding (Della Rosa et al., 2010; Paivio, 2013; Reggin et al., 2021; Wauters et al., 2003).  

Many researchers have underscored the role of language and social interaction in the acquisition, representation, and use of abstract concepts (Borghi, 2023; Dove, 2022). Some authors have also suggested a role for inner speech during abstract word processing (not only overt but also covert language) (Borghi & Fernyhough, 2023; Dove, 2019; Fini et al., 2021). Experimental (behavioral and fMRI) and rating studies implicate the involvement of the mouth motor system during abstract word processing, a finding that is consistent with the role of language in abstract meaning (Barca et al., 2017, 2020; Borghi et al., 2011; Borghi & Zarcone, 2016; Dreyer & Pulvermüller, 2018; Ghio et al., 2013). 

Abstract meanings are typically associated with more emotional/affective experience (Kousta et al., 2011; Lund et al., 2019; Newcombe et al., 2012; Ponari et al., 2018; Vigliocco et al., 2014), although not all abstract words are affect-laden. Similarly, abstract concepts, particularly emotional ones, are rated as evoking more inner and interoceptive experiences than concrete concepts (Connell et al., 2018; Lynott et al., 2020; Villani et al., 2021; Kelly et al. (in press)). In addition to language and emotion, abstract concepts are related to visual or motor experience, social constellations and mental states (Harpaintner, Sim, Trumpp, Ulrich, & Kiefer 2020, Kiefer, Pielke, & Trumpp, 2022, Ulrich, Harpaintner, Trumpp, Berger, Kiefer, 2022).  

Different subgroups of abstract concepts have been identified with a differential relevance of specific experiential or linguistic information (Harpaitner, Trumpp & Kiefer, 2018; Kiefer & Harpaintner 2020). Experiments investigating the use of abstract concepts reveal that people prefer starting a conversation with abstract concepts than with concrete concepts (Fini et al., 2023), that they evoke more metaphorical and beat gestures and more words referring to people and introspection (Zdrazilova et al., 2018), and more expressions referring to uncertainty and "why" questions (Villani et al., 2022), consistent with the higher uncertainty they generate. 

The meanings of different kinds of abstract concepts might be weighted differently in various dimensions and might have different, even if partially overlapping, neural underpinnings. For example, emotions and interoception might be more crucial for abstract emotional concepts. The kinds of abstract concepts more commonly identified in the literature are the following: Emotions; Numbers + spatiotemporal (magnitude); Social relations; Philosophical-spiritual; Theory of mind/mentalizing; Scientific abstract concepts (Catricalà et al., 2021; Conca, Borsa, et al., 2021; Conca, Catricalà, et al., 2021; Desai et al., 2018; Diveica et al., 2023, 2023; Kiefer & Harpaintner, 2020; Kiefer, Pielke, & Trumpp, 2022, Mazzuca et al., 2022; Muraki et al., 2020; Muraki, Sidhu et al., 2022; Primativo et al., 2016; Ulrich, Harpaintner, Trumpp, Berger, & Kiefer, 2022). 
```

###### Dissents
```{toggle}
*Bolognesi*: The investigation into whether abstract words lack the taxonomic hierarchical organization, a hallmark of many concrete word categories, is currently underway (see Villani et al., in press). Indeed, certain types of abstract concepts exhibit a greater degree of lexical granularity than others. For instance, within the realm of spiritual concepts, Catholicism can be classified as a type of Christianity, which, in turn, falls under the broader category of monotheistic religions, which is a type of religion, and so forth. Similarly, abstract words and concepts within other social reality domains demonstrate a notably conventionalized taxonomic structure. 

*Majid*: The contemporary definition of abstractness offered here uses criteria that could apply as well to concepts that typically would be identified as concrete. The proposed criteria are: (1) understood based on language, (2) draws from interoception, introspection, and metacognition, and (3) applies to “perceptually dissimilar actions and events”. Arguably all concepts rely on these criteria—for example, even concrete concepts can be perceptually dissimilar (cf. sexual dimorphism in the animal kingdom, e.g., duck, orangutan). For these criteria, it is unclear what would be excluded from the scope. It is also not obvious how to apply “understood primarily on the basis of language” across concepts or populations. Are visual concepts concrete for sighted individuals (because they are learned through perception) but abstract for blind people (because they are primarily learned via language input)? Are secondary color concepts (e.g., sepia, chartreuse) abstract because we learn about them through language use rather than ostension, but basic color concepts not abstract because they are learned under different conditions? For these reasons, the classic definition of abstract concepts that rests on opposition to the concrete definition is preferable—i.e., abstract concepts are those that are intangible and difficult to perceive directly through the senses.    
```


## Abstraction

##### Definition
The process of forming general ideas or concepts by extracting similarities and general tendencies from direct experience, language, or other concepts. 
###### Background
```{toggle} 
The term "abstraction" originated from the Latin word "abstractio" which is derived from the verb "abstrahĕre”, composed of two Latin elements: "ab," meaning "away" or "from," and "trahere," meaning "to draw" or "to pull". Therefore, the etymology of "abstraction" reflects the idea of pulling away or separating, emphasizing the cognitive process of distilling essential information or concepts from the complexities of reality. 

The term "abstraction" has a rich history with usage that has evolved over time. It can be traced back to ancient Greek philosophy, particularly to the works of Aristotle, who saw the process of abstraction as a way of understanding and categorizing the world. During the Renaissance, philosophers such as Descartes and Locke discussed the role of abstraction in forming general ideas (Laurence & Margolis, 2012; Murdoch et al., 1987). In the early 20th century, Vygotsky, Piaget, and Bruner studied the development of abstraction throughout childhood, casting abstraction as a fundamental cognitive process. Piaget distinguished between abstraction through associative learning (i.e., pattern and similarity detection) and abstraction through transformation of schema from lower to higher stages of cognitive development (Piaget, 2014). A similar distinction was advanced by French (1995), a computer scientist whose framework of analogy-making describes how different types of conceptual slippages correspond to either (i) abstraction of concrete instances to an abstract schema, (ii) abstraction via transportation of the schema across different situations, or (iii) abstraction that involves transformation of schema to align with a novel context. More recently, Barsalou (2003) identified six distinct types of abstraction, two of which refer to constructs defined elsewhere in this work (i.e., categorical knowledge—see Category/Categorization; abstract concepts—see Abstract/Abstractness), three of which describe the output of the process of abstraction (i.e., summary, schematic, and flexible representations), and finally one that (partially) covers the process we here consider: the ability to generalize across category members. 

Abstraction is similar to generalization (Colunga & Smith, 2003), with one difference being that abstraction refers to identifying essential features or properties to form a higher-level representation, whereas generalization refers to the process of transferring knowledge or skills from specific instances or exemplars to new contexts (Son et al., 2008). Abstraction should not be confused with Abstractness, even though the two variables are positively correlated (Bolognesi et al., 2020; Bolognesi & Caselli, 2023). In fact, abstraction processes can apply to the construction of both concrete and abstract concepts. 

Abstraction is often empirically assessed via tasks or measures that require participants (1) to identify common features or properties shared by a group of objects or events, (2) to generalize properties from known to novel items, and (3) to infer and apply abstract rules or schema. Examples include the following: 

Categorization tasks, which typically involve providing a set of instances to the participant who is asked to sort instances into categories or provide the category label of each instance. Such tasks are commonly used in developmental psychology research to investigate children’s categorization abilities (Gopnik & Meltzoff, 1987; Sloutsky & Fisher, 2004); although they are also used to study the nature of expertise by asking experts and novices to categorize physics problems (Chi et al., 1981) and to examine how variability within the category influenced categorizations by manipulating the amount of distortion from prototypical “grid images” that participants were later asked to categorize (Fried & Holyoak, 1984), among many other applications.  

Analogical reasoning tasks, where participants are given an incomplete analogy typically consisting of pairs of conceptual entities (e.g., bread: slice of bread: lemon:?) and have to complete the analogy (i.e., slice of lemon). Note that these conceptual entities do not necessarily need to be linguistic in nature—images of shapes or abstract patterns, or images of people and objects, have been commonly used (e.g., People Pieces Analogy Task (Rattermann & Gentner, 1998; Viskontas et al., 2004)). Success in this task relies on the ability to abstract out the common relations that apply to both domains (e.g., the second object is obtained by slicing the first with a knife) and applying the relation to infer the identity of the missing entity. 

Novel Noun Generalization tasks that involve showing an exemplar and labeling the exemplar (e.g., "This is a /dax/"). The participant is then shown other (novel) objects and asked which objects have the same name (i.e., is also a /dax/). The task measures how individuals generalize a category label to novel instances and is commonly used by developmental psychologists to study the emergence of conceptual categories in child development (Colunga & Smith, 2003; Landau et al., 1988; Soja et al., 1991). 

Problem solving tasks or tasks that involve higher order reasoning about physical systems (Schwartz & Black, 1996), mathematical concepts (Fyfe et al., 2014), or abstract sequences (Kemeny & Lukacs, 2019). The classic study by Schwartz and Black (1996) presented students with problems that led them to solve for the direction of the final gear in a sequence of turning gears and showed how students can cancan transition from a depictive model to inferring the abstract rule that could be used to solve future problems.  
```
###### Dissents
```{toggle}
none to date
```


## Action Semantics

##### Definition
Action semantics subsumes a collection of diverse neurocognitive representations engaged in meaningful action performance, manipulable object and action recognition, tool use, action categorization, and language about events involving actions.
###### Background
```{toggle} 
A diverse array of hierarchically structured neurocognitive representations support action semantics (Grafton & Hamilton, 2007). At lower levels of hierarchy, action semantic representations include embodied/grounded sensory (visuo-somatosensory-kinesthetic) information about how actions should look and feel. These representations subserve action performance and recognition, as well as knowledge of the actions relevant to manipulable objects (e.g., a hammer is used with an oscillating gesture that looks and feels a certain way). For example, the left intraparietal sulcus/supramarginal gyrus (IPL, SMG) and lateral occipital-temporal cortex (LOTC) support action retrieval during recognition of manipulable objects and actions (Garcea & Mahon, 2014; Chao & Martin, 2000; Raffaele et al., 2019). In motor production tasks (e.g., object use or meaningful gesture production), action semantic representations serve as “targets” that guide specific motor plans to achieve the desired sensory states for familiar actions. However, action semantic representations are not motor plans themselves. Rather, these representations include the range of actions that would accomplish the goal of, e.g., hammering and the typical actions performed within a given context. Action semantic representations at this embodied level are organized in terms of the similarity of their action features, such that representations with hand and arm trajectories that look and feel similar compete during retrieval (Watson & Buxbaum, 2014). These representations may be implicitly activated when manipulable objects are viewed (Lee, Middleton, Mirman et al., 2013), and are distinguishable from actions specified solely by the structural “affordances” of objects: the latter are calculated online and allow appropriate object grasping even when an object is unfamiliar and/or the skilled use associated with it is unknown. 
At higher levels of the hierarchy, action semantics include abstract causal and mentalistic representations of intentions and goals. Infants perceive actions as intentional and goal-directed within the first few months of life (Liu & Spelke, 2017; Pelphrey et al., 2005). Neural systems involved in action processing are sensitive to the unobservable intentional and causal structure of actions (Bi, 2021; Laurence & Margolis, 2012; Pelphrey et al., 2004). For example, neural response patterns in the right superior temporal sulcus (rSTS) are sensitive to the distinction between helping and hindering events, reflecting sensitivity to the agent’s social goals (Isik et al., 2017). Regions that respond to language about actions (i.e., action verbs), including the posterior left middle temporal gyrus (pLMTG) represent not only observable physical actions (e.g., running) but also invisible mental ones (e.g., thinking, wanting) and develop invariantly in the face of changes in sensory experience, such as congenital blindness or congenital absence of limbs (Bedny et al., 2008, 2012; Vannuscorps & Caramazza, 2016).  
Not all verbs refer to explicit actions (e.g., rusting, existing), and not all actions are strictly verbs (e.g., swimming is my favorite exercise). Verbs are fundamentally grammatical objects defined by their syntactic behavior in sentences, with morphological, argument structure, thematic, and morpho-phonological properties that are partially orthogonal to action semantics (Bird et al., 2000; McRae et al., 1997; Vigliocco et al., 2004). The neural basis of actions and verbs is partially dissociable (Arévalo et al., 2007; Damasio & Tranel, 1993; Hillis et al., 2004; Vigliocco et al., 2011), and thr mapping of actions to verbs varies cross-linguistically (e.g., cut-with-scissors and cut-with-knife are distinct basic level verbs in Dutch and Mandarin, see Majid et al., 2008). 
Action semantic representations at the two main levels of hierarchy interact dynamically during behavior. For example, during a motor action, such as swinging a golf club, the action goals and intentions are translated into the kinematics of the limb movements (Desai et al., 2018; Fernandino et al., 2016). Action semantic representations are not an ‘all-or-none’ phenomenon. That is, not all aspects of our knowledge of ‘give’ or ‘cut’ are retrieved every time an action or manipulable object is viewed or imagined (Lee et al., 2013). Rather, retrieval is influenced by contextual factors, including task goals, social communicative context, current bodily states, affordances, and other cues present in the environment (Xiong et al., 2023). 
```
###### Dissents
```{toggle}
*Papeo*: The investigation on the posterior superior temporal sulcus (pSTS) region that seems to selectively respond to, and discriminate between social interaction events (i.e., helping vs. hindering; Isik et al., 2017) is currently ongoing. In effect, discrimination has been reported during visual perception of events (i.e., helping and hindering) that systematically differ for visuo-spatial properties (e.g., spatial relations between actors, motion trajectories), leaving open the possibility that effects of “social goals” reflect visuo-perceptual, rather than semantic differences between action events (see Bellot et al., 2021; Pitcher & Ungerleider, 2021). The present observation also highlights a general difficulty in defining the boundary between semantic and perceptual representation, due to both methodological and conceptual limitations of the field (for recent discussions, see Hafri & Firestone, 2021; Hochmann & Papeo, 2022). 

*Majid*: There appears to be a categorical error in this definition of action semantics, which includes in it “language about events involving actions”. However, semantics is one component of language that deals with meaning. Other levels of analysis would include, for example, phonology and syntax. So to define action semantics as including language is a conflation of different linguistic levels. It is an open question—much debated—whether linguistic semantics and non-linguistic concepts are identical or at least partially distinct.    
```

## Concept

##### Definition
Concepts are coherent, relatively stable (but not static) units of knowledge in long-term memory that provide the elements from which more complex thoughts can be constructed. A concept captures commonalities and distinctions between a set of objects, events, relations, properties, and states. Concepts allow for the transfer and generalization of information without requiring explicit learning of every new instance.
###### Background
```{toggle}
The definition of ‘concept’ in contemporary cognitive neuroscience owes a great deal to Tulving’s (1972) conception of semantic memory as a common substrate for language processing and other cognitive activities. Researchers have offered various characterizations of how concepts serve this functional role. Eleanor Rosch’s pioneering research on the categorization of everyday objects (1973) framed human concepts as those that "provide maximum information with the least cognitive ability." Clark (1983) defines ‘concept’ as “a set of properties that are associated with each other in memory and thus form a unit.” Murphy (2002) proposes that, “Concepts are a kind of mental glue, then, in that they tie our past experiences to our present interactions with the world, and because the concepts themselves are connected to our larger knowledge structures.” While Medin and Coley (1998) write, “By concept we mean a mental representation of a category serving multiple functions, one of which is to allow for the determination of whether something belongs to the class. A category refers to the set of entities picked out by the concept.” They distinguish seven categories of functions: categorization, understanding, inference, explanation and reasoning, learning, communication, and combination.  

At its most basic, a concept is a mental representation that binds a class of stimuli to a certain treatment. Concepts can be verbal or non-verbal. Non-verbal animals, including human infants, exhibit concepts because they produce untrained responses to novel members of a common class, even when those class members are physically quite distinct (Carey, 2009; Gelman, 1996; Lazareva et al., 2004). For example, nine-month-old infants who discover that a toy wails when tipped will persist in tipping that object when it does not wail and will generalize their tipping action to distinct novel objects that share some properties with the toy but not to dissimilar objects (Baldwin et al., 1993). Preverbal and nonverbal concepts are sometimes called ‘equivalence classes’. An equivalence class is a subtype of ‘concept’ in which a group of distinct stimuli elicits a common behavioral response (Urcuioli, 2006). Many accounts of concept acquisition propose a continuum from concrete to abstract, or from similarity-based to theory-based, and these distinctions might be useful for characterizing concepts, but they do not neatly map onto stages of evolution, development, or linguistic knowledge (Gelman, 1996). 

Concepts are so central that they have been a subject of inquiry since ancient times. The classical theory of concepts, which dates back at least to the ancient Greeks, posited that concepts are definitions built from simpler concepts (e.g., bachelor = unmarried + man). However, a problem for the theory is that precise definitions do not exist for most concepts (e.g., what defines a game?) (Wittgenstein, 1953). Two influential cognitively oriented theories have avoided this problem by doing away with definitions: Prototype theory holds that concepts are probabilistic: for each concept (e.g., dog), a list of features is encoded (e.g., has four legs, has fur, barks) and weighted by how frequently it has occurred relative to the target concept in the past (see Rosch & Lloyd, 1978). In contrast, exemplar models not only avoid definitions, they also suggest that a stored list of features is unnecessary (Medin & Schaffer, 1978; Smith & Medin, 1981). Instead, to decide if something is, for example, a dog, we compare it to each of our previous experiences with dogs (stored in mental representations).  

Some have questioned whether the term, concept, picks out a productive scientific kind. Miller and Johnson-Laird write: “Concepts are invisible, impalpable, ill-defined abstractions that have a nasty way of being whatever a theorist needs them to be at the moment.” (1976, p.697). In a more cautious vein, Murphy (2002) notes, “Concepts may have a great variety of forms and contents, and this is part of what has made the field so complex.” In fact, much critique has focused on the overwhelming amount of attention in cognitive science and neuroscience to studying concepts with clear denotations (i.e., objects, events, relations) in contrast to those grounded in social systems (e.g., kinship, marriage, ownership), linguistic systems (e.g., tense, aspect, mood), or logical systems (e.g., conjunction, possibility, necessity). Machery (2009) argued for abandoning the nomenclature of ‘concept’ because the available evidence suggests that there are separate mechanisms associated with exemplars, prototypes, and theories. Less radically, some have suggested that researchers remain justified in using the term but may need to acknowledge that concepts can be complex hybrids (Edwards, 2011; Prinz, 2004). 

There have been long-standing debates concerning the flexibility of concepts. Concepts have traditionally been defined in terms of invariant default knowledge that exhibits three characteristic properties: rapid retrieval, automaticity, and context-independence (Machery, 2015). Barsalou (1983) proposed that concepts encompass both context-independent and context-dependent properties. More recently, many researchers have proposed that concepts are flexibly shaped by task and context (Barsalou, 2016; Casasanto & Lupyan, 2015; Connell & Lynott, 2014; Kuhnke et al., 2021; Yee & Thompson-Schill, 2016; Hoenig, Sim, Bochev, Herrnberger, Kiefer 2008). 
```
###### Dissents
```{toggle}
none to date
```

## Concrete/Concreteness

##### Definition
1) (historical) The extent to which a word or concept evokes an experience grounded within the five Aristotelian basic senses (e.g., vision, audition, olfaction, gustation, tactition) (sense as referenced by Locke, 1685). This historical perspective was often used categorically in reference to the distinction between abstract and concrete knowledge; 
2) (contemporary) the extent to which a word or concept evokes a (multi)sensory experience encompassing both the classical basic senses but also extending to the chemical senses, interoception, and sense of self (e.g., body awareness and related phenomena). 
###### Background
```{toggle}
References to the distinction between abstract and concrete words are pervasive throughout the histories of linguistics and western philosophy. Modern empirical efforts at measuring and controlling for concreteness effects first involved asking young people to provide subjective ratings of words using Likert scales. These foundational methods were advanced by Alan Paivio (1926-2016) and his many colleagues and collaborators.  

The historical definition of concreteness referenced above was derived from the original rating scale reported by Paivio et al (1968), asking participants to rate the extent to which a word can be experienced through the senses. This operational definition of concreteness served as the gold standard for a vast body of research on concreteness and imageability effects over the subsequent half century (Breedin et al., 1994; Cousins et al., 2018; Hoffman & Lambon Ralph, 2011; Papagno et al., 2009; Plaut & Shallice, 1993; Sadoski & Paivio, 1994; Schwanenflugel & Stowe, 1989). Concreteness ratings are typically derived via Likert scale ratings reflecting a continuous range of sensory salience rather than a dichotomization of abstract or concrete. For many cognitive scientists today, the meaning of concreteness has evolved to include a wider range of sensory experiences, including sensations initiated within the body (e.g., hunger, emotional pain, interoception). The traditional dichotomy of concreteness as a marker of sensory salience has been replaced with a deeper understanding of abstract words having their own unique representational content (for a critique, see Shallice & Cooper, 2013). One of the challenges involved in manipulating concreteness as an independent variable is the historical drift of this construct and its variable interpretation across different fields (e.g., educational psychology). Since concreteness comes with centuries of historical baggage, some researchers have recently moved toward alternative measures of sensorimotor salience (Connell & Lynott, 2012; Muraki, Siddiqui, et al., 2022; Pexman et al., 2019). 
```
###### Dissents
```{toggle}
*Hoffman*: There are two separate issues at stake in this definition. The first is a measurement issue: What criteria do researchers use to determine how concrete a word is? Unlike many of the constructs defined in this article, concreteness has long been quantified through large-scale rating studies (as has its cousin, imageability). Most language research uses one of these sets of ratings to index concreteness, providing a common operational basis for the construct. Major studies collecting concreteness ratings have used definitions that emphasize the senses through which we experience the external world. For example, Brysbaert et al.’s (2014) ratings for 40,000 English lemmas used the instructions: “A concrete word…refers to something that exists in reality; you can have immediate experience of it through your senses (smelling, tasting, touching, hearing, seeing) and the actions you do.” (see page 906) Instructions do not typically mention chemical senses, proprioception or sense of self as determinants of concreteness. Therefore, I would argue that the historical definition is, in practice, what most researchers are using to operationalize concreteness in contemporary research. 
The second issue is what types of experience are central to the meanings of the words that people classify as concrete. Here, the contemporary definition acknowledges a growing understanding that experiences of our own internal states (physical, cognitive. and emotional) contribute to semantic representation (Barsalou, 2016; Kiefer & Harpaintner, 2020; Vigliocco et al., 2014). However, it is far from clear that these types of experience are particularly associated with concrete words, as conventionally defined. In fact, many researchers have argued that interoceptive and emotional experiences are more prominent in the representations of abstract words (see Abstractness definition). Ultimately, this debate illustrates the difficulty in reducing the complexity of sensory experience to a single unidimensional construct. Multi-dimensional approaches may offer a more nuanced way forward (Binder et al., 2016; Connell & Lynott, 2012; Crutch et al., 2013).  
*Reilly*: Although participants are typically given explicit instructions on how they should rate concreteness, words such as ‘pain’ ‘spicy’ and ‘smelly’ that index interoceptive or chemosensory states are in fact relatively high in rated concreteness (as are words such as ghost and spirit).  One possibility is that words whose meanings are salient in one modality (e.g., hunger) evoke strong contextual associations with concrete words. This phenomenon is evident when people describe odors by anchoring their meaning to source emitters (e.g., ‘smells like a skunk’). For this reason, I favor the more expansive sense of concreteness as denoting any bodily experienced sensation. Thus, most words are at least somewhat concrete with relatively few exceptions (e.g., the, a, any).  
*Majid*: A concrete concept has historically been defined as one that is tangible and perceived directly through the senses. While the five-sense model of perception does not accurately reflect our current scientific understanding of the senses, it is important to note that the addition of the “chemical senses” in the contemporary definition has precedent since smell and taste are chemical senses. The background section states that “the meaning of concreteness has evolved to include a wider range of sensory experiences, including sensations initiated within the body (e.g., hunger, emotional pain, interoception)”, but these interoceptive states were also used to define abstractness in the earlier definition, making it unclear how these should be used by researchers to identify concrete vs abstract concepts. One remedy would be to maintain the classic definition “perceived directly through the senses” while acknowledging our expanding understanding of the senses to include chemesthesis, proprioception (both of which would have been included under classic ‘touch’), etc.  The sense of self is distinct, however, since as well as including some perceptual elements, it also includes notions of self-awareness, personal identity, consciousness, etc. which arguably are not concrete and should not be used to define concreteness. 
*Bolognesi*: The operationalization of concreteness by means of concreteness ratings has limitations, some of which are described here. A (in my opinion) major limitation that is not mentioned here is the fact that such ratings are typically collected by showing words in isolation, decontextualized from language use. Research has shown that this is problematic especially for polysemous words that have a very concrete and a very abstract sense (Reijnierse et al., 2019), like “side” (concrete surface of an object) and “side” (abstract argumentative standpoint), where both meanings are quite frequent and salient in speakers mind. In fact, “side” has a medium concreteness score with a fairly high standard deviation, suggesting that judgments about the two senses of “side” are conflated in the final concreteness score. Recent studies have started to release datasets of concreteness ratings collected on words shown in context (Montefinese et al., 2023), tackling the different senses outlined above. However, there are other contextual factors that impact the perceived concreteness of a concept, including the actions that are performed with it: is the concreteness of “apple” the same, when we read “I imagined an apple” and “I bit an apple”? 
```

## Embodied Cognition versus Grounded Cognition

##### Definition
1) (historical) Embodied cognition holds that cognitive functions depend on bodily experiences. In the specific field of semantic cognition, embodied cognition claims that words and concepts are acquired and represented via bodily experiences (i.e., perception and action). 
2) (contemporary) Embodied cognition refers to theories claiming that concepts exclusively comprise sensory and motor features represented and processed in modality-specific sensory and motor brain regions. Grounded cognition is the theory that concepts contain perceptual and motor features represented and processed in modality-specific perceptual and motor brain regions. Perceptual features may include internal states such as interoception or emotion, in addition to external sensations. Grounded cognition theories often assume that modality-specific features are complemented by more abstract cross-modal representations. 
###### Background
```{toggle}
Embodied and grounded cognition are related terms often used interchangeably. Both embodied and grounded cognition emphasize a crucial role of the human body in conceptual knowledge representation and processing (Pulvermüller, 1999; Barsalou, 2008). Embodied and grounded cognition offer a compelling solution to the so called “symbol grounding problem” (faced by amodal theories) that symbols, such as words, can be thought of as empty shells until their meaning is linked to a concrete perceptual or motor referent (Harnad, 1990; Searle, 1980). Grounding (also referred to as symbol grounding or perceptual grounding) specifically refers to symbolic systems such as language where the meanings of words are reified or grounded through bodily experiences (Searle, 1980). 

To clearly distinguish the terms “embodied cognition” and “grounded cognition,” we propose to restrict “embodied cognition” to “strong embodiment,” the view that concepts consist exclusively of sensory and motor features that are represented and processed in modality-specific sensory and motor brain regions (Gallese and Lakoff, 2005). Note that these modality-specific regions could be higher-level association areas of modality-specific perceptual-motor systems, not necessarily primary sensory-motor cortices (Fernandino et al., 2016; Kiefer et al., 2023).  

In contrast, grounded cognition theories are broader and often incorporate internal perceptual modalities, such as introspection, emotion, and mentalizing (Kiefer and Harpaintner, 2020; Vigliocco et al., 2014). Moreover, many grounded cognition theories do not restrict the conceptual system to modality-specific areas but allow for the additional involvement of cross-modal brain regions that integrate modality-specific features into more abstract conceptual representations (Binder and Desai, 2011; Fernandino et al., 2016; Kuhnke et al., 2020, 2023; Simmons and Barsalou, 2003). The latter theories are often also called “hybrid theories” as they incorporate elements from classical embodied cognition theories (i.e., perceptual-motor features represented in modality-specific perceptual-motor areas) and amodal theories (i.e., more abstract, cross-modal features represented in cross-modal convergence zones) (Kiefer and Pulvermüller, 2012; Dove, 2023). 
```
###### Dissents
```{toggle}
*Yee*: This dissent is merely about the insertion of the word “exclusively” in the contemporary definition of embodied cognition. In particular, the definition states: “Embodied cognition refers to theories claiming that concepts exclusively [emphasis added] comprise sensory and motor features represented and processed in modality-specific sensory and motor brain regions”. Including “exclusively” in this definition turns it into what is often called the “strong” version of embodied cognition (as the background notes). However, I believe that many readers understand the term “embodied cognition” to be a more general one that (by itself) is silent with respect to whether it refers to “strong” or “weak” embodiment (“weak” embodiment allows for the inclusion of components of concepts that are processed elsewhere). More importantly, for those who are new to the field and who may be using the definitions in this article as a guide, I fear that it will create confusion if they attempt to read the existing literature with the view that “embodied cognition” specifically refers to strong embodiment.   
I do agree that more clarity is needed regarding what exactly we mean when we use the term “embodied cognition”, as there is certainly a lack of consensus. In fact, in contrast to the definition above, it has been suggested that the “latent majority” view is the weak version (Zwaan, 2014). However, rather than restricting use of the term to cases in which we mean “strong embodiment” (how will we know whether authors are adhering to this?), I suggest that we use explicit language like “a strong version of embodied cognition” or “strong embodiment” when that is what we mean. To give a perhaps clearer example, convincing people to restrict their use of the word “car” to only cases when they mean “red car” would be challenging indeed.    
```

## Event Semantics

##### Definition
Event semantics focuses on the perceptual, motor, conceptual, and linguistic representations of events, which, in contrast to objects, typically pertain to how individual entities and the relations between entities persist or change over time. It includes how the continuous flow of experience is segmented into discrete events, with beginnings and endings, along with hierarchical organization.
###### Background
```{toggle}
The linguistics literature on event semantics focuses on how events are represented by words and sentences, and because this literature is both large and heterogeneous, for the present purposes we will list some of the main research topics, since they reflect strong consensus about critical themes. First, a common goal is to determine the most empirically and theoretically coherent way to decompose linguistic representations of events into configurations of semantic features. Some commonly posited basic elements of event structure include AGENT, PATIENT, INSTRUMENT, GOAL, ACT, CAUSE, GO, MANNER, PATH, BE, PLACE, HAVE, BECOME, and STATE. Second, it is widely agreed that there are three broad aspectual types of events: activities, which lack an inherent endpoint (e.g., walk); achievements, which denote the instant at which a state is attained (e.g., win a race); and accomplishments, which extend over time and culminate in a result state (e.g., draw a circle). Third, numerous fine-grained classes and subclasses of event-denoting verbs have been distinguished by a combination of syntactic and semantic criteria. For example, verbs of "breaking" and verbs of "hitting" can both be used in transitive sentences (e.g., The boy broke/hit the window with a rock), but only the former can be used in intransitive sentences with undergoer subjects (e.g., The window broke/*hit). This is because verbs of "breaking" are pure CHANGE OF STATE verbs, whereas verbs of "hitting" encode MOTION followed by CONTACT without entailing a state change. Fourth, related to the previous point, an important aim is to develop semantic explanations of argument structure alternations, which involve different syntactic realizations of similar event structures. Examples include the dative alternation (e.g., Bob gave a ring to Sue / Bob gave Sue a ring), the locative alternation (e.g., Bob loaded hay onto the truck / Bob loaded the truck with hay), and the body-part possessor alternation (e.g., Bob bumped Sue's arm / Bob bumped Sue on the arm). Fifth, another popular topic concerns the generalized semantic/thematic roles that event participants play. Examples include agent (or actor), patient (or undergoer), experiencer, recipient, and instrument. Sixth, all the topics mentioned above, among many others, are investigated in hundreds of languages around the world, often with the goal of identifying cross-linguistic similarities and differences in the representation of events. 

The neuroscientific investigation of event semantics aims to explain how events are represented and mapped in the mind/brain. In the following, we identify the main topics of research, concerning different, central aspects of event semantics. First, the study of event semantics in psychology, psycholinguistics, and cognitive and developmental psychology has addressed the universal components of events as a window into the conceptual categories of the human mind. Events are associated with several properties that do not apply to objects. Among them, research has highlighted types of events (e.g., causation, motion, change of state and transfer), temporal properties (e.g., starting moment, ending moment, duration), changes in properties of entities (e.g., size, shape, colour, position) or in interactions between entities, and thematic or semantic roles (e.g., agent, patient, goal and instrument), which determine the role of entities in an event and their relation (Rissman & Majid, 2019). How the mind/brain codes event-specific properties, also in relation to sensory, perceptual and motor representations (Papeo, 2020; Kominsky & Scholl, 2020; Strickland & Scholl, 2015), is a focus of current research. Second, the study of event segmentation addresses how the continuous flow of phenomenological experiences is segmented into discrete units, which can be hierarchically structured, with brief, fine-grained events aggregated into extended, coarse-grained events (Kurby & Zacks, 2008; Radvansky & Zacks, 2011). Event segmentation involves shared representations in memory, language, and perception and involves the integration of information on multiple, concurrent timescales. A recent paper (Yates et al., 2023) identifies three main frameworks that have been developed to explain event segmentation: “events as objects,” which emphasizes the similarities between events and (visual) objects; “events as the consequences of prediction error,” which emphasizes the role of prediction in event segmentation; and “events as inferred causal structure,” which focuses on the top-down influence of internal models in event segmentation. Together with the investigation of event boundaries, researchers are now asking questions about the specific contents of events, that is, the parts that are contained within those boundaries (spatio-temporal context, people, goals, states, emotions, etc., and the relationships among them). Third, given that actions are a prominent category of events, the study of event semantics has been informed by the study of behavioural and neural correlates of action and verb processing (Wurm & Caramazza, 2022). Action observation and understanding has been found to consistently implicate a network of occipitotemporal and frontoparietal regions, sometimes called the action observation network. While researchers have generally focused on single action events with human agents acting in isolation, more recent work is exploring the networks associated with other types of events like social interactions and natural (i.e., agentless) events. Fourth, research on infants’ cognition investigates  the intuitions or expectations that infants have about physical and psychological events, how infants acquire knowledge about events, which aspects of events are privileged in the infant’s mental representation, and how understanding events relates to the sensorimotor experience in the environment (Baillargeon & Wang, 2002; Gergely & Csibra, 2003). Finally, events are fundamental to human experience, as they constitute the stream of experience, the things that are remembered or forgotten in autobiographical memory, and the components of our plans for future action. For this reason, the study of event semantics naturally overlaps with research on perception and sensory-motor processes, episodic and autobiographical memory, and affective neuroscience. Challenges in the study of event semantics primarily reflect the lack of a unified definition of what is an event, i.e., what constitutes an event for an individual and what parts of experience matter. According to recent perspectives (Yates et al., 2023), progress can come from a radical rethinking of what an event is and from recognizing that events are not one thing that can be captured by a single definition, but many things, which may need to be studied separately.
```
###### Dissents
```{toggle}
*Fedorenko*: My primary objection to the consensus definition of ‘event semantics' concerns the inclusion of ‘perceptual,’ ‘motor,’ and ‘linguistic’ representations, in addition to conceptual representations. I use the term to refer selectively to language-independent and abstract (not tied to perception and motor control), i.e. conceptual, representations of events. 
The reason for separating conceptual representations (for events and more generally) from 1) perception and motor control and 2) linguistic processing is that empirically conceptual representations dissociate from both. First, although we may engage perceptual and motor machinery to process certain kinds of object or event, it is well established that there exist conceptual representations that are independent of perceptual and motor processing. The strongest evidence for the existence of such representation comes from individuals with drastically different perceptual and motor experiences (e.g., individuals who are born blind or without limbs). Despite these experiential differences, these individuals appear to end up with conceptual representations that are remarkably similar to those of individuals with access to the full range of perceptual and motor experiences, as measured using both behavioral approaches (e.g., Bedny et al., 2019; Kim et al., 2019, 2021; Vannuscorps & Caramazza, 2020) and brain imaging (e.g., Bedny et al., 2012; Sriem-Amit et al., 2018; Wang et al., 2020; see Bedny et al., 2008 for complementary fMRI evidence from participants with a full range of perceptual and motor experiences; see Bedny & Caramazza, 2011 for a review). This body of evidence suggests that perceptual and motor systems are not critical to acquiring conceptual knowledge and representing concepts of objects and events. 
And second, linguistic and conceptual (or semantic; I use these terms interchangeably) processing dissociate (again, for events specifically and more generally). At least three sources of evidence support this dissociation. First, pre-linguistic infants represent events and make complicated inferences about how agents interact with objects and how objects and agents interact with each other long before they learn words for the constituent event participants and relationships between them (e.g., Hirsh-Pasek & Golinkoff, 2006; Spelke, 2023). Second, some individuals with even severe aphasia (linguistic deficits) lose the ability to interpret and generate linguistic descriptions of objects and events, but retain the ability to understand the world (e.g., Chertkow et al., 1997; Saygin et al., 2004; Antonucci & Reilly, 2008; Warren & Dickey, 2021), including making sophisticated judgments about event plausibility, likely event orders, and so on (e.g., Varley & Siegal, 2020; Dickey & Warren, 2015; Colvin et al., 2019; Ivanova et al., 2021). In contrast, conceptual representations can be impaired in other patient populations (e.g., semantic dementia) in the presence of intact linguistic abilities (e.g., Jefferies & Lambon Ralph, 2006; Lambon Ralph et al., 2010). Third, in brain imaging studies, distinct sets of brain areas are activated selectively by linguistic event descriptions versus in an amodal fashion by both linguistic and non-linguistic (e.g., visual-pictorial) event representations (Baldassano et al., 2018; Wurm & Caramazza, 2019; Ivanova, 2021). 
My secondary objection is with the second sentence. A multitude of research questions have been asked and are being asked about how events are represented and processed; it seems peculiar for a general definition to single out a particular research direction (dealing with event segmentation). 
*Majid*: As with “action semantics” the inclusion of “linguistic representations” to define “semantics” is a conflation of distinct components of language. Event semantics should include within its scope issues of meaning but not, for example, phonology and syntax. So to define action semantics as including language is a conflation of different levels of linguistic analysis. As Federenko points out, there are reasons we would want to maintain a distinction between linguistic and non-linguistic semantics, minimally so we can at least ask as scientists whether these involve identical or distinct representations.   
```

## Lexical Semantics

##### Definition
Lexical semantics refers to the system of conventionalized meanings of linguistic forms in a language. A linguistic form is a sequence of speech sounds (spoken language), manual signs (sign language), visual symbols (orthographic writing systems), or tactile symbols (braille script), or abstractions over these sequences (e.g., sequences of phonemes, graphemes, syllables, morphemes, or words). Lexical meanings can include concepts and relations, as well as other shades of meaning conventionally associated with linguistic forms, including affective (e.g., positive or negative sentiment) and social (e.g., class, region, status) information. 
###### Background
```{toggle}
Lexical semantics concerns conventionalized form-meaning associations in natural language. The mental system that is thought to represent these form-meaning associations is typically called the lexicon. Mappings between linguistic form and meaning in the lexicon are ambiguous and underspecified: a single form can map onto multiple meanings, and a single meaning can map onto multiple forms. For example, a homonymous word like bank has different unrelated meanings (e.g., side of a river vs. financial institution), whereas a polysemous word like paper has different related meanings (e.g., piece of paper, newspaper, the building where the publisher sits, etc.). The lexicon is structured by systematic relationships that hold between the meanings of lexical entries. These relationships include hypernymy (category), hyponymy (category member), meronymy (part-whole relationship), association, syntagmatic (co-occurrence), and paradigmatic (ability to exchange words in a sentence, often based on synonymy, antonymy, or hypernymy). 

Lexical semantics differs from conceptual semantics in its specific focus on language, as opposed to other representational modalities (e.g., pictorial) by which meanings can be conveyed and mentally represented. Thus, the meanings of linguistic forms can include not only the concepts to which they refer, but also affective information (e.g., the meanings of dog and cur differ primarily in affect), social information (e.g. the meanings of guy and gentleman differ primarily in social register), or related shades of meaning that are specifically associated with linguistic forms (e.g., words) and not inherent to the concepts picked out by those forms. Lexical semantics differs from combinatorial semantics in its specific focus on linguistic meanings that are stored in memory, rather than derived or inferred from context. 

There are many outstanding questions about the mental lexicon that this consensus definition aims to avoid taking a position on. These questions include the granularity of stored linguistic forms (i.e., whether the lexicon contains morphemes, words, multiword expressions, or some combination of these; e.g., Katz & Postal, 1963; Fiorentino & Poeppel, 2007); the contents of lexical meaning representations (e.g., Cruse, 1986); the format of representations in the lexicon (e.g., Coltheart, 2004); the amount of redundancy in the lexicon (i.e., whether derivable form-meaning relationships can also be stored, e.g., Taft & Forster, 1976); the relationship between the lexicon and the broader semantic system (e.g., Jackendoff, 1993; Sowa, 1993); and the relationship between the lexicon and the grammar (e.g., Goldberg, 1995; Bybee, 1998; Marantz, 1997). 
```
###### Dissents
```{toggle}
none to date
```

## Modal/modality

##### Definition
1) (historical) From psychophysiology: a specific sensory channel (e.g., color is typically a visual modality); From linguistics: the representational format of any information channel (e.g., newspapers are a print modality);
2) (contemporary) Any discrete channel for transmitting, receiving, and/or representing information including but not limited to primary sensory data. For morphological derivatives of modality (e.g., amodal, heteromodal), we recommend indexing/aligning meanings with common dictionary definitions of these prefixes (e.g., a-, pan-, trans-, hetero-). 
###### Background
```{toggle}
Modality is among the most common, yet ambiguous terms used in semantic research. For example, many researchers trained in neuroscience and perception link modality with sensory data. That is, modality typically references a discrete sensory channel (e.g., visual modality, auditory modality). In other disciplines such as linguistics, modality is often used in reference to the representational format of a particular stimulus (e.g., newspapers as a print modality). The challenge of converging upon a broad consensus for modality is that many subdisciplines of cognitive science have cultivated theories premised on their own unique interpretations of this term (for a recent discussion and alternate proposal for disambiguation see Raia, 2023). The contemporary definition of modality proposed here represents an amalgamation of perspectives. Namely, we propose that modality references any discrete information channel for either the transmission or representation of information including but not limited to primary sensory data. Thus, vision and orthography could both be considered modalities. Vision is a sensory modality, whereas orthography is a representational modality. Vision and print are both channels dedicated to either receiving or transmitting information. For clarity we suggest that sensory modality be consistently used when limiting to primary sensory data, and representational modality used when any dimension (not limited to sensory data) is intended (for a distinction between input modality and representational modality, see Kiefer, Kuhnke, & Hartwigsen, 2023). 

The term modality has numerous morphological derivatives. Many of these constructs have featured prominently in a longstanding debate over semantic organization in the human brain. Proponents of embodied theories hold that semantic memory is grounded in modality-specific systems distributed across sensory and motor cortices (Machery, 2016; Patterson & Lambon Ralph, 2016; Hoffman & Lambon Ralph, 2013; Rogers, Lambon Ralph, Garrard, et al., 2004; Jefferies et al., 2010). Another prominent perspective holds that semantic knowledge is mediated by amodal symbols (Hoffman et al., 2018; Machery, 2016; Patterson et al., 2007; Patterson & Lambon Ralph, 2016).  

Inflected derivatives of modality often index semantic phenomena in opaque ways that diverge from standard dictionary definitions (see Calzavarini, 2023). For example, an unfamiliar researcher might assume that amodal means ‘no modality’ since the English morpheme a- typically denotes away from, lacking, or without (e.g., asexual, atheist, amoral). However, this is not always the case. 

Descriptions of commonly used derivatives of modality follow: 
* Amodal: Not directly tied to physical aspects of the environment (e.g., not topographically organized).  
* Crossmodal: Includes processing from two or more modalities, often referring to perceptual processes occurring within the brain. For example, auditory cortex is typically responsive to both auditory and visual speech information. 
* Heteromodal: Synonym for multimodal (see multimodal). 
* Modality-invariant: Areas of the brain or of a semantic space that are recruited for a particular target concept regardless of its sensory or representational modality. 
* Modality-specific: (Syn: unimodal) responding to one and only one modality. 
* Modality-preferential: Responding more to one modality than others (but may still show a response to more than one modality, in contrast with modality-specific). 
* Multimodal: Responding to and integrating across more than one sensory and/or representational modality. 
* Polymodal: Synonym for multimodal (see multimodal). 
* Supramodal: Synonym for amodal (see amodal). 
* Transmodal: Synonym for modality-invariant. 
```
###### Dissents
```{toggle}
*Bolognesi*: From my perspective, ambiguity around the meaning(s) of modality is growing, as evident in recent debates in cognitive semiotics and cognitive linguistics (e.g., Stampoulidis, 2020; Bolognesi & Werkmann Horvat 2023: 105-107). Printed newspaper articles (the example mentioned in the definition) use primarily written linguistic signs to convey meaning. Print engages visual sensory channels converging upon modality-specific representations in mind. Therefore, the distinction between sensory modality and representational modality does not resolve ambiguity associated with ‘modality’ because the term “representation” is itself also ambiguous and can refer to both the semiotic system in which a message is expressed and its corresponding conceptual representation. 

Rather than representational and sensory modality, a better distinction would be between: 1) semiotic systems of expression to define the system of signs through which a message is conveyed—often the research focus of semiotic and linguistic approaches—and 2) sensory modalities to refer to the channels through which messages are processed—often the research focus of cognitive scientific approaches. 

*Bi*: I oppose defining modal/modality to include both sensorimotor and representational components. While sensory modalities of the brain are clear-cut (for the human brain, sensory: vision, audition, haptic, olfactory, taste; motor), what constitutes a representational “modality” is highly debatable and open-ended. Using modality to also refer to the latter is counterproductive. It would be clearer to follow the neuroscientific convention to use modal/modality for sensory channels, and use other ways to clarify the different types of representational contents (e.g., “representational content” or “information content”). That is, visual modality would mean the visual sensory channel, which can convey information computing various types of content such as shape, color and texture of objects or forms of written language (orthography). 
```

## Semantic Control

##### Definition
The set of executive control processes that regulate the activation and deployment of semantic knowledge. These allow flexible, context- and task-appropriate responses by ensuring that only relevant aspects of semantic representations are used to direct thought and behavior. 
###### Background
```{toggle}
The contemporary study of semantic control emerged from neuropsychological studies of “semantic access” (Campanella et al., 2009; Warrington & Shallice, 1979) and “refractory access” deficits (Warrington & Cipolotti, 1996; Warrington & Crutch, 2004). This work led to the establishment of a double dissociation between deficits of semantic control in semantic aphasia versus long-term conceptual knowledge representation in semantic dementia (Jefferies & Lambon Ralph, 2006). People with semantic aphasia have difficulty regulating semantic cognition across verbal and non-verbal tasks in different contexts (e.g., resolving lexical ambiguity in the context of distractors). Semantic aphasia is typically associated with left hemisphere cerebrovascular accidents impacting frontoparietal and/or posterior temporal lobe regions (Thompson et al., 2022).  

Early neuropsychological studies implicating semantic control later converged with functional neuroimaging studies demonstrating parametric modulation (upregulation) of left inferior frontal gyrus (LIFG) during executively demanding semantic tasks. In a seminal study, Thompson-Schill and colleagues (1997) reported LIFG activity mediated by competition between active semantic representations. The authors proposed that this region mediated top-down selection of relevant semantic knowledge. Badre & Wagner (2002) later argued that LIFG was engaged in effortful retrieval of semantic knowledge as well as competition resolution. These authors coined the term “semantic control” to refer to these processes. It is important to note, however, that LIFG damage is not universally associated with difficulty resolving lexical-semantic competition (Britt et al., 2016). Further, fMRI studies have identified a distributed network of regions that are sensitive to semantic control demands, including LIFG (including a large swathe of pars triangularis, orbitalis and opercularis) and left posterior temporal cortex (including posterior superior temporal sulcus, middle temporal gyrus and inferior temporal gyrus) (Noonan, Jefferies, Visser & Lambon Ralph 2013, Jackson, 2021). Neurostimulation of these regions affects ambiguity resolution and the efficiency with which weak associations can be retrieved (Davey et al., 2015; Whitney et al., 2011). Additionally, neuroimaging studies consistently identify similar effects in bilateral dorsomedial prefrontal cortex (centered on presupplementary motor area) and right IFG (Jackson, 2021), although these areas have received less attention in the literature.  

What is semantic control and why do we need it? Representations of a concept consist of a multitude of features and associations which are unlikely to all be applicable to the current situation, and indeed some may directly counteract the current aim. For instance, we normally think of dogs as friendly family pets but if we encounter one accompanying a security guard, this dominant information will not support appropriate behavior. Semantic control processes act on semantic representations in a top-down fashion to shape activation in the semantic system and produce a conceptual structure that suits the needs of the current context (Zhang et al., 2021). Context can include an individual’s current task or goal but also the wider situation in which processing is taking place (e.g., linguistic or environmental context). Semantic control processes are integral to the normal operation of the semantic system and are assumed to be engaged to some extent in all forms of semantic processing. However, they are most essential when automatic stimulus-driven processing alone does not lead to the context-appropriate aspects of meaning becoming most strongly activated (Wang et al., 2020). There are two sets of circumstances which have been investigated. 

First, when automatic stimulus-driven processing fails to activate context-relevant knowledge, controlled retrieval processes are thought to engage a more effortful, internal search for relevant semantic information (Badre & Wagner, 2002; Hoffman, 2018). This may be important, for example, when people need to access less frequent meanings of ambiguous words or to search for novel or less salient associations between concepts. The second case is when multiple semantic representations (i.e., multiple concepts or features) are strongly activated and compete to influence behavior. Here, semantic selection processes are thought to boost the activation of context-appropriate representations and inhibit those that are not currently relevant (Jackson et al., 2021). This may be important, for example, when people make decisions based on specific properties of concepts or simply when tailoring their responses to only include a particular subset of information. Mechanistic accounts of these controlled retrieval (Hoffman et al., 2018) and semantic selection (Jackson et al., 2021) processes can simulate both typical and impaired semantic control. Although these are inter-connected processes, there is some evidence that they may have distinct neural and behavioral correlates (Badre et al., 2005). However, the degree to which they are served by distinct neural systems remains an open question. 

Current research is focusing on how semantic control processes relate to domain-general executive functions (e.g., Chiou et al., 2023; Gao et al., 2021; e.g., Humphreys & Lambon Ralph, 2017) and to cognitive control in episodic memory tasks (Vatansever et al., 2021). The left-lateralized semantic control network only partially overlaps the multiple-demand network of regions that respond to increased control demands across a broad set of cognitive domains (Jackson, 2021). Moreover, the key regions demonstrated to be necessary for semantic control—LIFG and posterior middle temporal gyrus—are not part of the core multiple-demand network, suggesting they play a more specialized role in regulating activation of semantic knowledge. Notably, these regions have highly left-lateralized patterns of connectivity (Alam et al., 2019) unlike the more bilateral multiple-demand network. Directly comparing the neural correlates of semantic and domain-general control while separating the effects of difficulty, task and stimulus type will be critical to understand to what extent these processes are neurally and computationally distinct. 

Finally, most work on semantic control has used manipulations of verbal stimuli, and much less is known about how the regulation of non-verbal knowledge is achieved. People with aphasia who have concurrent semantic control deficits have also been reported to experience parallel deficits in regulating object use, suggesting shared control processes for verbal and non-verbal knowledge (Corbett et al., 2009). However, the relatively large lesions in such cases could mean that patients had sustained damage to neighboring but distinct systems. Few fMRI studies have investigated semantic control demands in non-verbal semantic tasks (but see Krieger-Redwood et al., 2015) and this is a key target for future research. In addition, within the verbal domain, regions implicated in semantic control do not appear to be engaged in the control of executively demanding phonological processing (Hodgson & Lambon Ralph, 2021; Snyder et al., 2007). This suggests that semantic control cannot simply be equated to control over all verbal stimuli. 

Experimental manipulations of semantic control typically involve some combination of reducing the accessibility of task-relevant semantic knowledge while increasing the salience of irrelevant knowledge. For example, accessing less frequent meanings of ambiguous words is thought to place high demands on semantic control, both because the required knowledge is unlikely to be activated automatically during word processing and because strong activation of more dominant meanings must be inhibited. Tasks with similar demands include presenting multiple comparison stimuli (typically words) to probe knowledge for weak semantic associations and feature selection tasks where participants match items based on specific properties (e.g., color) while ignoring irrelevant semantic associations. 
```
###### Dissents
```{toggle}
none to date
```

## Semantic Dimension

##### Definition
Any variable used for differentiating exemplars (e.g., axe vs. spoon) across any given aspect of meaning (e.g., capacity for inflicting harm). Semantic dimensions are often but not always continuous (e.g., pleasantness vs. animacy). In high dimensional semantic space models, knowledge of the constituent semantic dimensions is essential for determining the coordinate location of any exemplar and computing its distance to all other exemplars.
###### Background
```{toggle}
Throughout the early 1970s to the present, cognitive scientists focused on semantic features in defining category boundaries and constraining word and object knowledge (McRae et al., 2005; Breedin et al., 1998; Caramazza & Shelton, 1998; Cree et al., 2006; Garrard et al., 2005; Rogers, Lambon Ralph, Hodges, et al., 2004). Semantic features typically reflect the binary presence or absence of a particular attribute (e.g., has fur, has a tail). The past decade has seen a new class of models premised upon characterizing concepts using many continuous dimensions such as color salience, arousal, or valence.  

The dimensions that comprise experiential semantic models are typically derived through subjective ratings. For example, the Lancaster Sensorimotor Norms reflect salience of dimensions such as color, olfaction, interoception, and hand/arm associations for tens of thousands of words as rated by many human participants (Lynott et al., 2020) and across many languages (Speed & Brysbaert, 2022; Chen, Zhao, Long, Lu, & Huang, 2019). Each of these continuous variables constitutes a single dimension. These individual factors are typically combined to form high dimensional semantic spaces (Banks & Connell, 2023; Binder et al., 2016; Crutch et al., 2013; Reilly et al., 2016, 2023).  

Word embeddings represent another high dimensional approach to specifying word meaning. However, the elements within vectors that comprise embedding models represent abstract mathematical constructs rather than meaningful psychological variables. For example, many embedding models characterize the meanings of words along 300 dimensions, none of which in isolation is psychologically analogous to semantic dimensions such as color or disgust that characterize more experiential semantic models (for discussion see Reilly et al., 2023). 
```
###### Dissents
```{toggle}
*Lupyan and Fedorenko*: Our primary objection with the current definition is the focus on the interpretability of the dimensions. There are two reasons for this objection. First, the interpretability of semantic dimensions does not seem to be a prerequisite for their success in capturing human semantic judgments. In particular, self-supervised word embedding models, like word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) are highly successful at capturing human behavioral data on semantic tasks (e.g., Pereira et al., 2016). At the core of these models is the idea that words that occur in similar linguistic contexts have similar meanings (Firth, 1957; Landauer, 1997). Some basic versions of word-embedding models can have interpretable dimensions, e.g., how often a given object concept co-occurs with a particular action verb like “eat” (Mitchell et al., 2008). The most general and successful versions of these models, however, involve computing 200-300 latent dimensions, which are no longer readily interpretable, although it seems possible to project them onto more interpretable dimensions such as concreteness, valence, and arousal. 
And second, interpretable semantic dimensions are generally limited: a) they tend to only characterize a subset of concepts (e.g., you one rate object concepts for animacy, but not a relational concept like ‘below’), b) are often generated by researchers in a somewhat ad hoc way (e.g., Binder et al., 2016) and, as a result, have baked into them potentially incorrect theoretical assumptions about the nature and organization of concepts, and c) require large amounts of human behavioral data, which makes them more difficult to generalize to new populations (e.g., individuals living different cultures, Blasi et al., 2022). The fact that these researcher-generated dimensions are easy to understand in no way implies that they reflect the true dimensional structure of people’s semantic space. 
As a result, we question the requirement of semantic dimensions to be a priori known or readily understandable. In fact, the dimensional structure obtained from more opaque methods like self-supervised word embedding models may turn out to be a better characterization of the human semantic space, and the phenomenal success of modern-day language models (Radford et al., 2019), which at their core rely on patterns of word co-occurrences, indeed suggests that this is likely to be the case. 
```

## Semantic Distance / Semantic Similarity

##### Definition
A quantitative measure of similarity/distance between two words (or concepts) situated within an n-dimensional semantic space.
###### Background
```{toggle}
Semantic features have a long tradition in both philosophy, psychology and computer science. Classical views (e.g., Aristotle) considered concepts as being defined by necessary and sufficient features, so that any given concept could be completely defined by providing the full list of its constituent features. In this way, semantic features can allow concepts to be structured into categories according to how their featural representations overlap. This idea was developed in work that viewed the human conceptual system in terms of taxonomic hierarchies (Collins & Quillian, 1969), and was further extended by more modern theories that built extensive concept-feature datasets, where semantic similarity between concepts could be derived by examining the extent of shared features between pairs of concepts (Malt & Smith, 1984). This led to further efforts to collate large-scale sets of semantic feature norms (McRae et al., 2005; Buchanan et al., 2019; Harpaintner, Sim, Trumpp, Ulrich, & Kiefer 2020), where participants would generate as many features as they could for individual concepts, providing list or vector-like representations for concepts and their features.  

Semantic features can be obtained relatively easily from non-specialists, with simple instructions to generate common properties of each concept in a list. In some cases, semantic features are obtained from experts, such as linguists, to build knowledge graphs or semantic networks such as WordNet (Miller, 1995). Once a set of concept-feature lists is collated, similarity between concepts can be calculated by methods such as the cosine of the angle between feature-frequency vectors. For example, two concepts with completely overlapping features would have a cosine similarity value of 1, while two concepts with no overlapping features would have a cosine similarity of 0. There is some debate regarding whether such featural similarity is the best way of estimating semantic similarity or whether alternative, non-featural methods are more effective (see also semantic space definition) (Wingfield & Connell, 2022). Nonetheless, the featural similarity approach makes semantic features useful when trying to investigate behavior related to phenomena in language processing (comprehension and production) and conceptual representation. Evidence for the utility of semantic features comes from a broad range of studies, from modelling semantic priming (Cree et al., 1999) and category-specific deficits (Tyler et al., 2000; Vinson et al., 2003; Warrington & Shallice, 1984), to investigating the source of false recognition memory (Montefinese et al., 2015). 

Semantic features have known limitations. Instructing participants to produce common properties for a concept prioritizes features which are more easily verbalized. As a result, feature lists are affected by the lexical specificity of a language and individual vocabularies of participants and might underestimate conceptual diversity among a group of speakers. Features are also easily generated for concrete nouns but less straightforward to verbalize for abstract nouns and other parts of speech such as verbs and adjectives. Collecting and norming feature lists is also labour intensive, meaning that coverage remains limited. For instance, the largest set of norms to date reported by Buchanan and colleagues (2019) compiles features for nearly 4,500 concepts, which—while extremely useful—is still well short of an adult-level vocabulary of approximately 40,000 words (Brysbaert et al., 2016). Consequently, it is unclear whether semantic features can in isolation provide a comprehensive picture of semantic memory (for critique of feature-based approaches see Jackendoff, 1987).  
```
###### Dissents
```{toggle}
none to date
```

## Semantic Feature

##### Definition
A component or element that relates to a concept or expresses a relation with other concepts. A concept can therefore be approximated as a collection of such features. Semantic features capture a wide range of information characteristics of a concept covering taxonomic relations, perceptual properties, function, behavior, thematic roles, and introspective features. Features are typically binary (present or absent for a concept) but can be weighted by criteria such as salience (e.g., [has wings] is important for BIRD) or context dependency (e.g., BIRD sometimes [is pretty]). Certain features also tend to co-occur among category members (e.g., [has wings], [has beak], [can fly]).
###### Background
```{toggle}
Semantic features have a long tradition in both philosophy, psychology and computer science. Classical views (e.g., Aristotle) considered concepts as being defined by necessary and sufficient features, so that any given concept could be completely defined by providing the full list of its constituent features. In this way, semantic features can allow concepts to be structured into categories according to how their featural representations overlap. This idea was developed in work that viewed the human conceptual system in terms of taxonomic hierarchies (Collins & Quillian, 1969), and was further extended by more modern theories that built extensive concept-feature datasets, where semantic similarity between concepts could be derived by examining the extent of shared features between pairs of concepts (Malt & Smith, 1984). This led to further efforts to collate large-scale sets of semantic feature norms (McRae et al., 2005; Buchanan et al., 2019; Harpaintner, Sim, Trumpp, Ulrich, & Kiefer 2020), where participants would generate as many features as they could for individual concepts, providing list or vector-like representations for concepts and their features.  

Semantic features can be obtained relatively easily from non-specialists, with simple instructions to generate common properties of each concept in a list. In some cases, semantic features are obtained from experts, such as linguists, to build knowledge graphs or semantic networks such as WordNet (Miller, 1995). Once a set of concept-feature lists is collated, similarity between concepts can be calculated by methods such as the cosine of the angle between feature-frequency vectors. For example, two concepts with completely overlapping features would have a cosine similarity value of 1, while two concepts with no overlapping features would have a cosine similarity of 0. There is some debate regarding whether such featural similarity is the best way of estimating semantic similarity or whether alternative, non-featural methods are more effective (see also semantic space definition) (Wingfield & Connell, 2022). Nonetheless, the featural similarity approach makes semantic features useful when trying to investigate behavior related to phenomena in language processing (comprehension and production) and conceptual representation. Evidence for the utility of semantic features comes from a broad range of studies, from modelling semantic priming (Cree et al., 1999) and category-specific deficits (Tyler et al., 2000; Vinson et al., 2003; Warrington & Shallice, 1984), to investigating the source of false recognition memory (Montefinese et al., 2015). 

Semantic features have known limitations. Instructing participants to produce common properties for a concept prioritizes features which are more easily verbalized. As a result, feature lists are affected by the lexical specificity of a language and individual vocabularies of participants and might underestimate conceptual diversity among a group of speakers. Features are also easily generated for concrete nouns but less straightforward to verbalize for abstract nouns and other parts of speech such as verbs and adjectives. Collecting and norming feature lists is also labour intensive, meaning that coverage remains limited. For instance, the largest set of norms to date reported by Buchanan and colleagues (2019) compiles features for nearly 4,500 concepts, which—while extremely useful—is still well short of an adult-level vocabulary of approximately 40,000 words (Brysbaert et al., 2016). Consequently, it is unclear whether semantic features can in isolation provide a comprehensive picture of semantic memory (for critique of feature-based approaches see Jackendoff, 1987).  
```
###### Dissents
```{toggle}
none to date
```

## Semantic Representation

##### Definition
The cognitive and neural manifestation of the information content of semantic knowledge, which is the structured knowledge stored in long-term memory (i.e., semantic memory). 
###### Background
```{toggle}
From the moment we are born and over the course of our lifetimes, we accumulate massive amounts of knowledge that encompasses knowledge of specific objects and entities (e.g., a cat or a chair), situations (e.g., a birthday party), abstract ideas (e.g., freedom), emotions (e.g., happiness), understanding of general facts (e.g., why people pay taxes) or social norms (e.g., what to wear to a wedding), as well as parts of our knowledge of the world that do not easily map onto a label or a verbal description (e.g., a particular spatial layout). Semantic representation refers to the currently active subset of this knowledge (the cognitive manifestation or thought about a specific component of semantic memory). The term can vary in its scope: it can be discrete or graded and it can refer broadly to an overarching subset of semantic knowledge about an aspect of the world, or more narrowly to a particular context-relevant feature of an object or event. 

Semantic representations (1) are short-lived (time-limited), (i2) can get activated by diverse perceptual inputs (a picture of a cat, the sound of a ‘meow,’ the smell of a litter box, etc.), linguistic inputs (the word “cat”), or internal thought processes (a memory of a childhood pet), and (i3) are often tailored to the demands of the current situation. For example, to decide whether a cat is smaller than a microwave when playing “Twenty Questions”, one needs to activate one’s semantic representations of a cat and a microwave, focusing on their sizes. Similarly, to decide whether to adopt a new cat, one needs to activate one’s semantic representation of a cat, but in this case, one may instead focus on the cuteness and cuddliness of cats or the fact that they shed and can scratch furniture. Thus, certain aspects of a semantic representation (i.e., perceptual, functional, situational, etc.) may be more or less salient in particular contexts (Hoenig, Sim, Bochev, Herrnberger, & Kiefer 2008; Kiefer & Pulvermüller, 2012). In this way, semantic representations can provide a type of interface that binds perception, action, language, and general knowledge. Sometimes, however, semantic representation can refer to context-independent thoughts that pertain to a subset of our world knowledge (e.g., our general knowledge about cats). 

Some, but not all, semantic representations are associated with verbal labels. In this case, individuals must have a mapping between labels and different subsets of semantic knowledge. Importantly, however, verbal labels are not part of the semantic representations. Instead, they constitute a separate, language-specific system that may function in parallel with the system that stores our world knowledge but is independent of it. For example, some animals, preverbal infants, individuals with no access to language (e.g., deaf individuals growing up without access to sign language), or individuals with aphasia who have lost access to labels can have semantic representations even though they do not have labels for them. (Note that some linguists and psycholinguists have used the term “semantic representation” to refer to representations of specifically linguistic meaning; we believe this usage can lead to confusion, and we therefore advocate abandoning this usage of the term.) 

We have here focused on the cognitive science perspective. Of course, particularly within cognitive neuroscience and cognitive neuropsychology approaches, semantic representations must also manifest as patterns of neural activity, but the 1:1 correspondence between the cognitive and neural manifestations of semantic representations remains debated.  
```
###### Dissents
```{toggle}
*Majid*: The definition provided specifies semantic representations are the “cognitive and neural manifestation” of knowledge which by definition rules out the possibility that artificial intelligence could have semantic representations. Rather than stipulating this to be the case, I believe this should be a matter of scientific inquiry.  In defense of maintaining, a distinction between linguistic “semantic representation” and non-linguistic “conceptual representation”, there are enough documented cases where these dissociate that it is relevant and helpful to maintain the separation.  
```

## Semantic Space

##### Definition
A latent topography bounded by different aspects of meaning (e.g., valence, arousal, animacy). Semantic spaces provide a coordinate system for situating target concepts and deriving their distances to other words or concepts. Embedding-derived semantic spaces typically distribute a target concept across numerous hyperparameters, whereas the dimensions that comprise experiential semantic spaces are psychologically meaningful in isolation (e.g., color, fear). Semantic spaces are often but not always neurobiologically plausible. 
###### Background
```{toggle}
Concepts are composed of many pieces of information, including features (e.g., ‘has a tail’) and dimensions (e.g., pleasantness). Semantic spaces provide a framework for decomposing words into vectors that capture meaning. The length of these vectors varies as a function of the dimensionality of the semantic space used to define them. For example, a two-dimensional semantic space constrained by valence and arousal would yield a vector of length two applied to any number of words. Although such low-dimensional semantic spaces are indeed possible, their utility is limited with respect to explaining real-world semantic phenomena (e.g., category-specific semantic impairments). 

Semantic spaces could be composed of a potentially infinite number of dimensions. An optimal semantic space would approximate the latent structure of semantic memory. Many efforts at specifying the dimensionality of semantic spaces involve educated guesses about which dimensions should be included that best account for semantic phenomena such as neuropsychological dissociations (Crutch et al., 2013) and decoding the meanings of words and utterances from multivariate brain imaging signals (Fernandino et al., 2022; Huth et al., 2012, 2016; Wang et al., 2020).  

Two broad classes of semantic space models have recently evolved. Experiential semantic models are characterized by psychologically meaningful dimensions (e.g., color, emotion) typically reflecting human subjective ratings (e.g., rate the extent to which this word makes you think of color) (Banks & Connell, 2023; Binder et al., 2016; Troche et al., 2017). In contrast, word embedding models yield high dimensional semantic spaces characterized by hundreds of hyperparameters generated from co-occurrence statistics in large natural language corpora. A word such as maze depicted in Figure 1 is represented by a 15-element vector in an experiential semantic model (SemDist15), whereas the semantic vector for maze generated by GLoVE (Pennington et al., 2014) would span 300 parameters. In experiential models, a researcher manually selects the dimensions, whereas the elements that comprise embedding models are abstract mathematical constructs agnostic to human intuition. The semantic distances generated by experiential and embedding models are strongly correlated, but it has been argued that they index different information about taxonomic and thematic semantic relationships (but see Grand et al., 2022; Reilly et al., 2023). 
```
###### Dissents
```{toggle}
none to date
```

## Simulation

##### Definition
Simulation is the pre-running or re-running of a process outside of the proximate context that normally compels or cues that process to run. Simulation can include input (perceptual), output (motor), and interoceptive (affective and cognitive) processes, and can be explicit and intentional or implicit and automatic. An example of explicit and intentional simulation is motor imagery or perceptual imagery. An example of implicit and automatic simulation would be perceptual activity during comprehension of sentences describing sensory events, or motor activity during observations of others’ actions (hand actions, speech). 
###### Background
```{toggle}
Simulation has played a prominent role in research within the embodiment framework—the idea that the format of abstract content is sensorimotor. Simulation is the mechanism that makes it possible for cognitive content and conceptual representations to plausibly be distributed over perceptual, motor, and cross-modal association systems. On such strong embodied views, conceptual content is the re-running of sensory, motor, and interoceptive processes that are engaged during perception, action, and interoceptive experience.  

Simulation has also been closely aligned with motor theories of perception, originally formulated in the speech domain (Liberman et al., 1967; Galantucci et al., 2006) and extended to manual actions by research on the mirror neuron system (Rizzolatti & Craighero, 2004). In both contexts, the idea is that motor processes corresponding to the perceived action are run automatically (and implicitly), and that the simulation of those motor processes constitutes (causally) part of the series of processes that constitute ‘recognition’ or ‘understanding.’  

Simulation is also used in a more abstract manner—where what is being simulated are abstract, more cognitively elaborate, and temporally extended representations, such as proprioceptive representations of the self (e.g., body schema), physical events (e.g., melting), and symbolic mathematical operations (Lakoff & Núñez, 2000; Rueschemeyer et al., 2010; Borghi & Cimatti, 2010). 

There is little debate that simulation is one of the ways the brain builds predictions about the body and the world, that it is a critical aspect of mental imagery (Decety, 1996; Moulton & Kosslyn, 2009), and that it can play a role in learning and memory (Liu et al., 2019). What is contentious is the role and function of simulation in supporting processes that were not traditionally thought to depend on a simulation. Representation and processing of conceptual content has been proposed to involve simulation of its corresponding perceptual, motor and interoceptive properties (Barsalou, 1999). The issue of boundary conditions for simulation has been intensely debated with respect to necessary vs. sufficient (or indeed epiphenomenal) contributions of simulation in conceptual processing. 

The primary evidence typically cited for simulation is thsat sensorimotor regions/representations are almost immediately activated during tasks that ‘should’ logically involve sensorimotor activity (e.g., picking, kicking, licking) (Hauk, 2016; Hauk et al., 2004; Pulvermüller et al., 2005; Barsalou, 2016; Meteyard et al., 2012). Evidence against a simulationist interpretation comes from neuropsychological patients with acquired brain lesions who have demonstrated sensorimotor impairments but do not show the concomitant conceptual-level impairments that would be predicted by some versions of a simulationist approach (Mahon et al., 2009; Sartori et al., 2007). Despite entrenched views on these primary sources of evidence, the issues are complex. There are numerous sources of counterevidence for both views. Causal evidence for simulationist approaches (Möttönen & Watkins, 2009) and computational models show that modality-independent conceptual representations can arise in systems that are based on perceptual-motor simulation (Chen et al., 2017). 
```
###### Dissents
```{toggle}
none to date
```